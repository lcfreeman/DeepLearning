{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21490d1d",
   "metadata": {},
   "source": [
    "#### Positional Encoding\n",
    "$$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,max_len,dim_model,device):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len,dim_model,device=device,dtype=torch.float32)\n",
    "        self.encoding.requires_grad=False\n",
    "        pos = torch.arange(0,max_len,device=device,dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,dim_model,2,device=device,dtype=torch.float32)*(math.log(10000)/dim_model))\n",
    "        self.encoding[:,0::2] = torch.sin(pos/div_term)\n",
    "        self.encoding[:,1::2] = torch.cos(pos/div_term)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        _, seq_len = x.size()\n",
    "        \n",
    "        return self.encoding[:seq_len,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba14272",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b72976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self,num_embeddings,embedding_dim,padding_idx):\n",
    "        super(TokenEmbedding,self).__init__(num_embeddings,embedding_dim,padding_idx)\n",
    "#相当于nn.Embedding(num_embeddings=num_embeddings,embedding_dim=embedding_dim,padding_idx=padding_idx,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9485d6",
   "metadata": {},
   "source": [
    "### TransformerEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc30909",
   "metadata": {},
   "source": [
    "<img src=\"./data/Transformer_figure/PosEmb.png\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06816e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self,max_len,dim_model,vocab_size,padding_idx,drop_prob,device):\n",
    "        super().__init__()\n",
    "        self.pos = PositionalEncoding(max_len=max_len,dim_model=dim_model,device=device)\n",
    "        self.emb = TokenEmbedding(num_embeddings=vocab_size,embedding_dim=dim_model,\n",
    "                                  padding_idx=padding_idx)\n",
    "        self.drop = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x_pos = self.pos(x)\n",
    "        x_emb = self.emb(x)\n",
    "        \n",
    "        return self.drop(x_pos+x_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e088a",
   "metadata": {},
   "source": [
    "##### ScaledDotProductAttention\n",
    "<img src=\"./data/Transformer_figure/SDPA.png\" width=\"200\" height=\"200\">\n",
    "$$\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac {QK^{T}} {\\sqrt{d_{k}}})V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287721f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    input q,k,v shape=[batch_size,num_heads,seq_len,split_dim_model]\n",
    "    output v shape=[batch_size,num_heads,seq_len,split_dim_model]\n",
    "           score shape=[batch_size,num_heads,seq_len,seq_len]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        d_k = k.size(-1)\n",
    "        k_t = k.transpose(2,3)\n",
    "        score = (q@k_t)/math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.to(torch.float32)\n",
    "            score = score.masked_fill(mask==0,-1e6)\n",
    "        score = self.softmax(score)\n",
    "        v = score@v\n",
    "        \n",
    "        return v,score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811642da",
   "metadata": {},
   "source": [
    "#### MultiheadAttention\n",
    "<img src=\"./data/Transformer_figure/MHA.png\" width=\"200\" height=\"200\">\n",
    "$$\\mathrm{MultiHead}(Q,K,V)=\\mathrm{Concat}(\\mathrm{head_1},...,\\mathrm{head_n})W^O$$\n",
    "$$\\mathrm{head_i}=\\mathrm{Attention}(QW_i^Q,KW_i^K,VW_i^V)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d453917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self,dim_model,num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.w_q = nn.Linear(dim_model,dim_model)\n",
    "        self.w_k = nn.Linear(dim_model,dim_model)\n",
    "        self.w_v = nn.Linear(dim_model,dim_model)\n",
    "        self.w_o = nn.Linear(dim_model,dim_model)\n",
    "    \n",
    "    def forward(self,q,k,v,mask):\n",
    "        q = self.w_q(q)\n",
    "        k = self.w_k(k)\n",
    "        v = self.w_v(v)\n",
    "        q = self.split(q)\n",
    "        k = self.split(k)\n",
    "        v = self.split(v)\n",
    "        out, attention = self.attention(q,k,v,mask)\n",
    "        out = self.concat(out)\n",
    "        out = self.w_o(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def split(self,x):\n",
    "        batch_size, seq_len, dim_model = x.size()\n",
    "        split_dim_model = dim_model//self.num_heads\n",
    "        \n",
    "        return x.reshape(batch_size,self.num_heads,seq_len,split_dim_model)\n",
    "    \n",
    "    def concat(self,x):\n",
    "        batch_size, num_heads, seq_len, split_dim_model = x.size()\n",
    "        x = x.transpose(1,2)\n",
    "        \n",
    "        return x.reshape(batch_size,seq_len,num_heads*split_dim_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040407b1",
   "metadata": {},
   "source": [
    "#### LayerNorm\n",
    "$$y=\\frac{x-\\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x]+\\epsilon}}*\\gamma+\\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99301db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,dim_model,epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim_model))\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(-1,keepdim=True)\n",
    "        var = x.var(-1,unbiased=False,keepdim=True)\n",
    "        \n",
    "        return (x-mean)/torch.sqrt(var+self.epsilon)*self.gamma+self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a7f81",
   "metadata": {},
   "source": [
    "#### PWFFN\n",
    "$$\\mathrm{FFN}(x)=\\mathrm{max}(0,xW_1+b_1)W_2+b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2abea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PWFFN(nn.Module):\n",
    "    def __init__(self,dim_model,ffn_hidden,drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_model,ffn_hidden)\n",
    "        self.fc2 = nn.Linear(ffn_hidden,dim_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213b134",
   "metadata": {},
   "source": [
    "### TransformerEncoderLayer\n",
    "<img src=\"./data/Transformer_figure/EncoderLayer.png\" width=\"200\" height=\"200\">\n",
    "$$\\mathrm{LayerNorm}(x+\\mathrm{Sublayer}(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061cf390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,dim_model,num_heads,ffn_hidden,drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention =  MultiheadAttention(dim_model=dim_model,num_heads=num_heads)\n",
    "        self.pwffn = PWFFN(dim_model=dim_model,ffn_hidden=ffn_hidden,drop_prob=drop_prob)\n",
    "        self.norm1 = LayerNorm(dim_model=dim_model)\n",
    "        self.norm2 = LayerNorm(dim_model=dim_model)\n",
    "        self.drop1 = nn.Dropout(p=drop_prob)\n",
    "        self.drop2 = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self,src,src_mask):\n",
    "        # layer1\n",
    "        x1 = src\n",
    "        q,k,v = src,src,src\n",
    "        out1 = self.attention(q,k,v,src_mask)\n",
    "        out1 = self.drop1(out1)\n",
    "        out1 = self.norm1(x1+out1)\n",
    "        \n",
    "        # layer2\n",
    "        x2 = out1\n",
    "        out = self.pwffn(x2)\n",
    "        out = self.drop2(out)\n",
    "        out = self.norm2(x2+out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488210b",
   "metadata": {},
   "source": [
    "## TransformerEncoder\n",
    "<img src=\"./data/Transformer_figure/Encoder.png\" width=\"200\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,max_len,dim_model,num_heads,num_layers,ffn_hidden,vocab_size,padding_idx,drop_prob,device):\n",
    "        super().__init__()\n",
    "        self.posemb = TransformerEmbedding(max_len=max_len,dim_model=dim_model,vocab_size=vocab_size,\n",
    "                                           padding_idx=padding_idx,drop_prob=drop_prob,device=device)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(dim_model=dim_model,num_heads=num_heads,\n",
    "                                     ffn_hidden=ffn_hidden,drop_prob=drop_prob) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self,src,src_mask):\n",
    "        src = self.posemb(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src,src_mask)\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7d4e8",
   "metadata": {},
   "source": [
    "### TransformerDecoderLayer\n",
    "<img src=\"./data/Transformer_figure/DecoderLayer.png\" width=\"200\" height=\"200\">\n",
    "$$\\mathrm{LayerNorm}(x+\\mathrm{Sublayer}(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self,dim_model,num_heads,ffn_hidden,drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention1 = MultiheadAttention(dim_model=dim_model,num_heads=num_heads)\n",
    "        self.attention2 = MultiheadAttention(dim_model=dim_model,num_heads=num_heads)\n",
    "        self.pwffn = PWFFN(dim_model=dim_model,ffn_hidden=ffn_hidden,drop_prob=drop_prob)\n",
    "        self.norm1 = LayerNorm(dim_model=dim_model)\n",
    "        self.norm2 = LayerNorm(dim_model=dim_model)\n",
    "        self.norm3 = LayerNorm(dim_model=dim_model)\n",
    "        self.drop1 = nn.Dropout(p=drop_prob)\n",
    "        self.drop2 = nn.Dropout(p=drop_prob)\n",
    "        self.drop3 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self,enc_src,tgt,src_mask,tgt_mask):\n",
    "        # layer1\n",
    "        x1 = tgt\n",
    "        q,k,v = tgt,tgt,tgt\n",
    "        out1 = self.attention1(q,k,v,tgt_mask)\n",
    "        out1 = self.drop1(out1)\n",
    "        out1 = self.norm1(x1+out1)\n",
    "        \n",
    "        # layer2\n",
    "        x2 = out1\n",
    "        q = out1\n",
    "        k,v = enc_src,enc_src\n",
    "        out2 = self.attention2(q,k,v,src_mask)\n",
    "        out2 = self.drop2(out2)\n",
    "        out2 = self.norm2(x2+out2)\n",
    "        \n",
    "        # layer3\n",
    "        x3 = out2\n",
    "        out = self.pwffn(out2)\n",
    "        out = self.drop3(out)\n",
    "        out = self.norm3(x3+out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69338194",
   "metadata": {},
   "source": [
    "## TransformerDecoder\n",
    "<img src=\"./data/Transformer_figure/Decoder.png\" width=\"200\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99dec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,max_len,dim_model,num_heads,num_layers,ffn_hidden,vocab_size,padding_idx,drop_prob,device):\n",
    "        super().__init__()\n",
    "        self.posemb = TransformerEmbedding(max_len=max_len,dim_model=dim_model,vocab_size=vocab_size,\n",
    "                                           padding_idx=padding_idx,drop_prob=drop_prob,device=device)\n",
    "        self.layers = nn.ModuleList(TransformerDecoderLayer(dim_model=dim_model,num_heads=num_heads,\n",
    "                                    ffn_hidden=ffn_hidden,drop_prob=drop_prob) for _ in range(num_layers))\n",
    "        self.generator = nn.Linear(dim_model,vocab_size)\n",
    "        \n",
    "    def forward(self,enc_src,tgt,src_mask,tgt_mask):\n",
    "        tgt = self.posemb(tgt)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(enc_src,tgt,src_mask,tgt_mask)\n",
    "        out = self.generator(tgt)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f993b30",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "<img src=\"./data/Transformer_figure/Model.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec59cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,max_len,dim_model,num_heads,num_layers,ffn_hidden,src_vocab_size,src_padding_idx,tgt_vocab_size,tgt_padding_idx,drop_prob,device):\n",
    "        super().__init__()\n",
    "        self.src_padding_idx = src_padding_idx\n",
    "        self.tgt_padding_idx = tgt_padding_idx\n",
    "        self.encoder = TransformerEncoder(max_len=max_len,dim_model=dim_model,num_heads=num_heads,\n",
    "                       num_layers=num_layers,ffn_hidden=ffn_hidden,vocab_size=src_vocab_size,\n",
    "                       padding_idx=src_padding_idx,drop_prob=drop_prob,device=device)\n",
    "        self.decoder = TransformerDecoder(max_len=max_len,dim_model=dim_model,num_heads=num_heads,\n",
    "                       num_layers=num_layers,ffn_hidden=ffn_hidden,vocab_size=tgt_vocab_size,\n",
    "                       padding_idx=tgt_padding_idx,drop_prob=drop_prob,device=device)\n",
    "        \n",
    "    def forward(self,src,tgt):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        enc_src = self.encoder(src,src_mask)\n",
    "        out = self.decoder(enc_src,tgt,src_mask,tgt_mask)\n",
    "        \n",
    "        return out\n",
    "    # make mask\n",
    "    def make_src_mask(self,src):\n",
    "        src_mask = (src!=self.src_padding_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        return src_mask\n",
    "    \n",
    "    def make_tgt_mask(self,tgt):\n",
    "        tgt_pad_mask = (tgt!=self.tgt_padding_idx).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        tgt_seq_mask = torch.tril(torch.ones(tgt_seq_len,tgt_seq_len)).type(torch.ByteTensor).to(device)\n",
    "        tgt_mask = tgt_pad_mask & tgt_seq_mask\n",
    "        \n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb3aa5",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make mask\n",
    "def make_src_mask(src,src_pad_idx):\n",
    "    src_mask = (src!=src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    return src_mask\n",
    "def make_tgt_mask(tgt,tgt_pad_idx):\n",
    "    tgt_pad_mask = (tgt!=tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    tgt_seq_len = tgt.size(1)\n",
    "    tgt_seq_mask = torch.tril(torch.ones(tgt_seq_len,tgt_seq_len)).type(torch.ByteTensor).to(device)\n",
    "    tgt_mask = tgt_pad_mask & tgt_seq_mask\n",
    "    return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_src = torch.Tensor([\n",
    "    [9,4,3,2,5,6,1,1],\n",
    "    [5,3,2,6,8,4,1,1],\n",
    "    [2,3,4,5,6,7,8,9],\n",
    "    [5,6,7,4,8,6,2,1]\n",
    "])\n",
    "virtual_tgt = torch.Tensor([\n",
    "    [10,6,4,3,9,7,8,5,1,1],\n",
    "    [5,7,4,2,8,6,9,3,1,1],\n",
    "    [3,5,2,6,7,4,11,9,8,10],\n",
    "    [5,7,2,6,4,9,3,5,6,1]\n",
    "])\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "virtual_src,virtual_tgt = virtual_src.long().to(device),virtual_tgt.long().to(device)\n",
    "virtual_src_voc_size = 10\n",
    "virtual_tgt_voc_size = 11\n",
    "virtual_batch_size = 4\n",
    "virtual_dmodel = 6\n",
    "virtual_num_layers = 3\n",
    "virtual_nheads = 2\n",
    "virtual_ffn_hidden = 24\n",
    "virtual_max_len = 100\n",
    "virtual_drop_prob=0.1\n",
    "virtual_src_pad_idx=1\n",
    "virtual_tgt_pad_idx=1\n",
    "virtual_tgt_sos_idx=2\n",
    "#virtual_src_mask = make_src_mask(virtual_src,virtual_src_pad_idx)\n",
    "#virtual_tgt_mask = make_tgt_mask(virtual_tgt,virtual_tgt_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b04b27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = Transformer(virtual_max_len,virtual_dmodel,virtual_nheads,virtual_num_layers,virtual_ffn_hidden,\n",
    "                  virtual_src_voc_size,virtual_src_pad_idx,virtual_tgt_voc_size,virtual_tgt_pad_idx,\n",
    "                  virtual_drop_prob,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a457df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(virtual_src,virtual_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446684a",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657bd8a",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81265364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "# model parameter setting\n",
    "batch_size = 128\n",
    "max_len = 256\n",
    "dim_model = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "ffn_hidden = 2048\n",
    "drop_prob = 0.1\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# optimizer parameter setting\n",
    "init_lr = 1e-5\n",
    "factor = 0.9\n",
    "adam_eps = 5e-9\n",
    "patience = 10\n",
    "warmup = 100\n",
    "epoch = 1000\n",
    "clip = 1.0\n",
    "weight_decay = 5e-4\n",
    "inf = float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import collections\n",
    "import time\n",
    "import sacrebleu\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '../data/fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c288ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(raw_data_path,'r',encoding='utf-8')as f:\n",
    "    raw_data = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403f884",
   "metadata": {},
   "source": [
    "def get_char_set(raw_data):\n",
    "    src_char_list,tgt_char_list=[],[]\n",
    "    for item in raw_data:\n",
    "        item_list = item.strip('\\n').split('\\t')\n",
    "        src_char_list+=list(item_list[0])\n",
    "        tgt_char_list+=list(item_list[1])\n",
    "        \n",
    "    return set(src_char_list),set(tgt_char_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0967fc",
   "metadata": {},
   "source": [
    "get_char_set(raw_data)\n",
    "#'\\xa0','\\xad','\\u2009','\\u200b','\\u202f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a72eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train,raw_test = train_test_split(raw_data,test_size=0.1,train_size=0.9,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddeaf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self,token_model):\n",
    "        super().__init__()\n",
    "        self.tokenizer = spacy.load(token_model)\n",
    "    def tokenize(self,x):\n",
    "        return [token.text for token in self.tokenizer.tokenizer(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self,data_list,token_model,special_tokens,min_freq):\n",
    "        super().__init__()\n",
    "        self.tokenizer = Tokenizer(token_model)\n",
    "        token_list = [self.tokenizer.tokenize(item) for item in data_list]\n",
    "        token_list1d = [token for item in token_list for token in item]\n",
    "        counter = collections.Counter(token_list1d)\n",
    "        sort_counter = sorted(counter.items(),key=lambda x:x[1],reverse=True)\n",
    "        self.idx2token = special_tokens+[token for token,idx in sort_counter if idx>min_freq]\n",
    "        self.token2idx = {token:idx for idx, token in enumerate(self.idx2token)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2token)\n",
    "    \n",
    "    def __getitem__(self,tokens):\n",
    "        if not isinstance(tokens,(list,tuple)):\n",
    "            return self.token2idx.get(tokens,self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "        \n",
    "    def get_idx2token(self,idx):\n",
    "        if not isinstance(idx,list):\n",
    "            return self.idx2token[int(idx)]\n",
    "        return [self.get_idx2token(i) for i in idx]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token2idx.get('<unk>')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a974722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnFrDataset(Dataset):\n",
    "    def __init__(self,raw_data,src_tokenizer_model,tgt_tokenizer_model,special_tokens,min_freq):\n",
    "        super().__init__()\n",
    "        self.src_list, self.tgt_list = self.get_clean_data(raw_data)\n",
    "        self.src_vocab = Vocab(data_list=self.src_list,token_model=src_tokenizer_model,\n",
    "                               special_tokens=special_tokens,min_freq=min_freq)\n",
    "        self.tgt_vocab = Vocab(data_list=self.tgt_list,token_model=tgt_tokenizer_model,\n",
    "                               special_tokens=special_tokens,min_freq=min_freq)\n",
    "        self.src_pad_idx = self.src_vocab['<pad>']\n",
    "        self.tgt_pad_idx = self.tgt_vocab['<pad>']\n",
    "        self.tgt_sos_idx = self.tgt_vocab['<sos>']\n",
    "        self.tgt_eos_idx = self.tgt_vocab['<eos>']\n",
    "        \n",
    "    def __len__(self):\n",
    "        assert len(self.src_list)==len(self.tgt_list),'length is not equal!'\n",
    "        return len(self.src_list)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.src_list[idx],self.tgt_list[idx]\n",
    "    \n",
    "    def collate_fn(self,batch):\n",
    "        src_idx_list = [torch.tensor(self.src_vocab[self.src_vocab.tokenizer.tokenize(item[0])]) for item in batch]\n",
    "        tgt_idx_list = [torch.tensor([self.tgt_sos_idx]+self.tgt_vocab[self.tgt_vocab.tokenizer.tokenize(item[1])]\n",
    "                                     +[self.tgt_eos_idx]) for item in batch]\n",
    "       \n",
    "        src_padded = pad_sequence(sequences=src_idx_list,batch_first=True,padding_value=self.src_pad_idx)\n",
    "        tgt_padded = pad_sequence(sequences=tgt_idx_list,batch_first=True,padding_value=self.tgt_pad_idx)\n",
    "    \n",
    "        return src_padded,tgt_padded\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.src_vocab, self.tgt_vocab\n",
    "        \n",
    "    def get_clean_data(self, raw_data):\n",
    "        src_list = [raw_sentence.replace('\\u202f', ' ').replace('\\u2009', ' ').replace('\\u200b', ' ')\n",
    "                    .replace('\\xad', ' ').replace('\\xa0', ' ').lower().strip('\\n').split('\\t')[0] \n",
    "                    for raw_sentence in raw_data]\n",
    "        tgt_list = [raw_sentence.replace('\\u202f', ' ').replace('\\u2009', ' ').replace('\\u200b', ' ')\n",
    "                    .replace('\\xad', ' ').replace('\\xa0', ' ').lower().strip('\\n').split('\\t')[1] \n",
    "                    for raw_sentence in raw_data]\n",
    "        \n",
    "        return src_list, tgt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c436d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer_model = 'en_core_web_sm'\n",
    "tgt_tokenizer_model = 'fr_core_news_sm'\n",
    "special_tokens = ['<pad>','<unk>','<sos>','<eos>']\n",
    "min_freq = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf828dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EnFrDataset(raw_data=raw_train,src_tokenizer_model=src_tokenizer_model,min_freq=min_freq,\n",
    "                            tgt_tokenizer_model=tgt_tokenizer_model,special_tokens=special_tokens)\n",
    "test_dataset = EnFrDataset(raw_data=raw_test,src_tokenizer_model=src_tokenizer_model,min_freq=min_freq,\n",
    "                            tgt_tokenizer_model=tgt_tokenizer_model,special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6500e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset,batch_size=batch_size,collate_fn=train_dataset.collate_fn,shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset,batch_size=batch_size,collate_fn=test_dataset.collate_fn,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5737063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "src_voc,tgt_voc = train_dataset.get_vocab()\n",
    "src_pad_idx = src_voc['<pad>']\n",
    "tgt_pad_idx = tgt_voc['<pad>']\n",
    "tgt_sos_idx = tgt_voc['<sos>']\n",
    "enc_voc_size, dec_voc_size = len(src_voc),len(tgt_voc)\n",
    "len_train_dataloader = len(train_dataloader)\n",
    "len_test_dataloader = len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c007b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "\n",
    "\n",
    "model = Transformer(max_len,dim_model,num_heads,num_layers,ffn_hidden,enc_voc_size,src_pad_idx,dec_voc_size,\n",
    "                    tgt_pad_idx,drop_prob,device).to(device)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model.apply(initialize_weights)\n",
    "optimizer = optim.Adam(params=model.parameters(),\n",
    "                 lr=init_lr,\n",
    "                 weight_decay=weight_decay,\n",
    "                 eps=adam_eps)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 verbose=True,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, tgt) in enumerate(iterator):\n",
    "        src,tgt = src.to(device),tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output_reshape, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if (i+1)%100==0:\n",
    "            print(f'step : {(i+1)}/{len_train_dataloader}, {round((i / len_train_dataloader) * 100, 2)}% , loss : {loss.item()}')\n",
    "    return epoch_loss / len_train_dataloader\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    batch_bleu = []\n",
    "    with torch.no_grad():\n",
    "        for i, (src, tgt) in enumerate(iterator):\n",
    "            src,tgt = src.to(device),tgt.to(device)\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            total_bleu = []\n",
    "            for j in range(tgt.shape[0]):\n",
    "                tgt_sentence = ''.join(tgt_voc.get_idx2token(tgt[j].tolist()))\n",
    "                output_sentence = output[j].argmax(1)\n",
    "                output_sentence =''.join(tgt_voc.get_idx2token(output_sentence.tolist()))\n",
    "                bleu = sacrebleu.sentence_bleu(output_sentence,[tgt_sentence])\n",
    "                total_bleu.append(bleu.score)\n",
    "\n",
    "            total_bleu = sum(total_bleu) / len(total_bleu)\n",
    "            batch_bleu.append(total_bleu)\n",
    "            \n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output_reshape, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    batch_bleu = sum(batch_bleu) / len(batch_bleu)\n",
    "    return epoch_loss / len_test_dataloader, batch_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36716f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daea43b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(total_epoch, best_loss):\n",
    "    train_losses, test_losses, bleus = [], [], []\n",
    "    for step in range(total_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss = train(model, train_dataloader, optimizer, criterion, clip)\n",
    "        valid_loss, bleu = evaluate(model, test_dataloader, criterion)\n",
    "        end_time = time.time()\n",
    "\n",
    "        if step > warmup:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "        bleus.append(bleu)\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f'saved/model_{step+1}-{valid_loss}.pt')\n",
    "\n",
    "        #f = open('saved/train_loss.txt', 'w')\n",
    "        #f.write(str(train_losses))\n",
    "        #f.close()\n",
    "\n",
    "        #f = open('saved/bleu.txt', 'w')\n",
    "        #f.write(str(bleus))\n",
    "        #f.close()\n",
    "\n",
    "        #f = open('saved/test_loss.txt', 'w')\n",
    "        #f.write(str(test_losses))\n",
    "        #f.close()\n",
    "\n",
    "        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {math.exp(valid_loss):7.3f}')\n",
    "        print(f'\\tBLEU Score: {bleu:.3f}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run(total_epoch=epoch, best_loss=inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f711b68a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58b270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchlearn] *",
   "language": "python",
   "name": "conda-env-torchlearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
