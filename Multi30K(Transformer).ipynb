{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21490d1d",
   "metadata": {},
   "source": [
    "#### Positional Encoding\n",
    "$$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf9e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,max_len,dim_model,device):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len,dim_model,device=device,dtype=torch.float32)\n",
    "        self.encoding.requires_grad=False\n",
    "        pos = torch.arange(0,max_len,device=device,dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,dim_model,2,device=device,dtype=torch.float32)*(math.log(10000)/dim_model))\n",
    "        self.encoding[:,0::2] = torch.sin(pos/div_term)\n",
    "        self.encoding[:,1::2] = torch.cos(pos/div_term)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        _, seq_len = x.size()\n",
    "        \n",
    "        return self.encoding[:seq_len,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba14272",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b72976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self,num_embeddings,embedding_dim,padding_idx):\n",
    "        super(TokenEmbedding,self).__init__(num_embeddings,embedding_dim,padding_idx)\n",
    "#相当于nn.Embedding(num_embeddings=num_embeddings,embedding_dim=embedding_dim,padding_idx=padding_idx,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9485d6",
   "metadata": {},
   "source": [
    "### TransformerEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc30909",
   "metadata": {},
   "source": [
    "<img src=\"./data/Transformer_figure/PosEmb.png\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06816e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self,max_len,dim_model,vocab_size,padding_idx,drop_prob,device):\n",
    "        super().__init__()\n",
    "        self.pos = PositionalEncoding(max_len=max_len,dim_model=dim_model,device=device)\n",
    "        self.emb = TokenEmbedding(num_embeddings=vocab_size,embedding_dim=dim_model,\n",
    "                                  padding_idx=padding_idx)\n",
    "        self.drop = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x_pos = self.pos(x)\n",
    "        x_emb = self.emb(x)\n",
    "        \n",
    "        return self.drop(x_pos+x_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e088a",
   "metadata": {},
   "source": [
    "##### ScaledDotProductAttention\n",
    "<img src=\"./data/Transformer_figure/SDPA.png\" width=\"200\" height=\"200\">\n",
    "$$\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac {QK^{T}} {\\sqrt{d_{k}}})V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287721f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    input q,k,v shape=[batch_size,num_heads,seq_len,split_dim_model]\n",
    "    output v shape=[batch_size,num_heads,seq_len,split_dim_model]\n",
    "           score shape=[batch_size,num_heads,seq_len,seq_len]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        d_k = k.size(-1)\n",
    "        k_t = k.transpose(2,3)\n",
    "        score = (q@k_t)/math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.to(torch.float32)\n",
    "            score = score.masked_fill(mask==0,-1e6)\n",
    "        score = self.softmax(score)\n",
    "        v = score@v\n",
    "        \n",
    "        return v,score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811642da",
   "metadata": {},
   "source": [
    "#### MultiheadAttention\n",
    "<img src=\"./data/Transformer_figure/MHA.png\" width=\"200\" height=\"200\">\n",
    "$$\\mathrm{MultiHead}(Q,K,V)=\\mathrm{Concat}(\\mathrm{head_1},...,\\mathrm{head_n})W^O$$\n",
    "$$\\mathrm{head_i}=\\mathrm{Attention}(QW_i^Q,KW_i^K,VW_i^V)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d453917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self,dim_model,num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.w_q = nn.Linear(dim_model,dim_model)\n",
    "        self.w_k = nn.Linear(dim_model,dim_model)\n",
    "        self.w_v = nn.Linear(dim_model,dim_model)\n",
    "        self.w_o = nn.Linear(dim_model,dim_model)\n",
    "    \n",
    "    def forward(self,q,k,v,mask):\n",
    "        q = self.w_q(q)\n",
    "        k = self.w_k(k)\n",
    "        v = self.w_v(v)\n",
    "        q = self.split(q)\n",
    "        k = self.split(k)\n",
    "        v = self.split(v)\n",
    "        out, attention = self.attention(q,k,v,mask)\n",
    "        out = self.concat(out)\n",
    "        out = self.w_o(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def split(self,x):\n",
    "        batch_size, seq_len, dim_model = x.size()\n",
    "        split_dim_model = dim_model//self.num_heads\n",
    "        \n",
    "        return x.reshape(batch_size,self.num_heads,seq_len,split_dim_model)\n",
    "    \n",
    "    def concat(self,x):\n",
    "        batch_size, num_heads, seq_len, split_dim_model = x.size()\n",
    "        x = x.transpose(1,2)\n",
    "        \n",
    "        return x.reshape(batch_size,seq_len,num_heads*split_dim_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040407b1",
   "metadata": {},
   "source": [
    "#### LayerNorm\n",
    "$$y=\\frac{x-\\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x]+\\epsilon}}*\\gamma+\\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99301db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,dim_model,epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim_model))\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(-1,keepdim=True)\n",
    "        var = x.var(-1,unbiased=False,keepdim=True)\n",
    "        \n",
    "        return (x-mean)/torch.sqrt(var+self.epsilon)*self.gamma+self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a7f81",
   "metadata": {},
   "source": [
    "#### PWFFN\n",
    "$$\\mathrm{FFN}(x)=\\mathrm{max}(0,xW_1+b_1)W_2+b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2abea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PWFFN(nn.Module):\n",
    "    def __init__(self,dim_model,ffn_hidden,drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_model,ffn_hidden)\n",
    "        self.fc2 = nn.Linear(ffn_hidden,dim_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213b134",
   "metadata": {},
   "source": [
    "### TransformerEncoderLayer\n",
    "<img src=\"./data/Transformer_figure/EncoderLayer.png\" width=\"200\" height=\"200\">\n",
    "$$\\mathrm{LayerNorm}(x+\\mathrm{Sublayer}(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "061cf390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,dim_model,num_heads,ffn_hidden,drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention =  MultiheadAttention(dim_model=dim_model,num_heads=num_heads)\n",
    "        self.pwffn = PWFFN(dim_model=dim_model,ffn_hidden=ffn_hidden,drop_prob=drop_prob)\n",
    "        self.norm1 = LayerNorm(dim_model=dim_model)\n",
    "        self.norm2 = LayerNorm(dim_model=dim_model)\n",
    "        self.drop1 = nn.Dropout(p=drop_prob)\n",
    "        self.drop2 = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self,src,src_mask):\n",
    "        # layer1\n",
    "        x1 = src\n",
    "        q,k,v = src,src,src\n",
    "        out1 = self.attention(q,k,v,src_mask)\n",
    "        out1 = self.drop1(out1)\n",
    "        out1 = self.norm1(x1+out1)\n",
    "        \n",
    "        # layer2\n",
    "        x2 = out1\n",
    "        out = self.pwffn(x2)\n",
    "        out = self.drop2(out)\n",
    "        out = self.norm2(x2+out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488210b",
   "metadata": {},
   "source": [
    "## TransformerEncoder\n",
    "<img src=\"./data/Transformer_figure/Encoder.png\" width=\"200\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbf2a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,max_len,dim_model,num_heads,num_layers,ffn_hidden,vocab_size,padding_idx,drop_prob,device):\n",
    "        super().__init__()\n",
    "        self.posemb = TransformerEmbedding(max_len=max_len,dim_model=dim_model,vocab_size=vocab_size,\n",
    "                                           padding_idx=padding_idx,drop_prob=drop_prob,device=device)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(dim_model=dim_model,num_heads=num_heads,\n",
    "                                     ffn_hidden=ffn_hidden,drop_prob=drop_prob) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self,src,src_mask):\n",
    "        src = self.posemb(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src,src_mask)\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7d4e8",
   "metadata": {},
   "source": [
    "### TransformerDecoderLayer\n",
    "<img src=\"./data/Transformer_figure/DecoderLayer.png\" width=\"200\" height=\"200\">\n",
    "$$\\mathrm{LayerNorm}(x+\\mathrm{Sublayer}(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7bf4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self,dim_model,num_heads,ffn_hidden,drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention1 = MultiheadAttention(dim_model=dim_model,num_heads=num_heads)\n",
    "        self.attention2 = MultiheadAttention(dim_model=dim_model,num_heads=num_heads)\n",
    "        self.pwffn = PWFFN(dim_model=dim_model,ffn_hidden=ffn_hidden,drop_prob=drop_prob)\n",
    "        self.norm1 = LayerNorm(dim_model=dim_model)\n",
    "        self.norm2 = LayerNorm(dim_model=dim_model)\n",
    "        self.norm3 = LayerNorm(dim_model=dim_model)\n",
    "        self.drop1 = nn.Dropout(p=drop_prob)\n",
    "        self.drop2 = nn.Dropout(p=drop_prob)\n",
    "        self.drop3 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self,enc_src,tgt,src_mask,tgt_mask):\n",
    "        # layer1\n",
    "        x1 = tgt\n",
    "        q,k,v = tgt,tgt,tgt\n",
    "        out1 = self.attention1(q,k,v,tgt_mask)\n",
    "        out1 = self.drop1(out1)\n",
    "        out1 = self.norm1(x1+out1)\n",
    "        \n",
    "        # layer2\n",
    "        x2 = out1\n",
    "        q = out1\n",
    "        k,v = enc_src,enc_src\n",
    "        out2 = self.attention2(q,k,v,src_mask)\n",
    "        out2 = self.drop2(out2)\n",
    "        out2 = self.norm2(x2+out2)\n",
    "        \n",
    "        # layer3\n",
    "        x3 = out2\n",
    "        out = self.pwffn(out2)\n",
    "        out = self.drop3(out)\n",
    "        out = self.norm3(x3+out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69338194",
   "metadata": {},
   "source": [
    "## TransformerDecoder\n",
    "<img src=\"./data/Transformer_figure/Decoder.png\" width=\"200\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f99dec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,max_len,dim_model,num_heads,num_layers,ffn_hidden,vocab_size,padding_idx,drop_prob,device):\n",
    "        super().__init__()\n",
    "        self.posemb = TransformerEmbedding(max_len=max_len,dim_model=dim_model,vocab_size=vocab_size,\n",
    "                                           padding_idx=padding_idx,drop_prob=drop_prob,device=device)\n",
    "        self.layers = nn.ModuleList(TransformerDecoderLayer(dim_model=dim_model,num_heads=num_heads,\n",
    "                                    ffn_hidden=ffn_hidden,drop_prob=drop_prob) for _ in range(num_layers))\n",
    "        self.generator = nn.Linear(dim_model,vocab_size)\n",
    "        \n",
    "    def forward(self,enc_src,tgt,src_mask,tgt_mask):\n",
    "        tgt = self.posemb(tgt)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(enc_src,tgt,src_mask,tgt_mask)\n",
    "        out = self.generator(tgt)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f993b30",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "<img src=\"./data/Transformer_figure/Model.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec59cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,max_len,dim_model,num_heads,num_layers,ffn_hidden,src_vocab_size,src_padding_idx,tgt_vocab_size,tgt_padding_idx,drop_prob,device):\n",
    "        super().__init__()\n",
    "        self.src_padding_idx = src_padding_idx\n",
    "        self.tgt_padding_idx = tgt_padding_idx\n",
    "        self.encoder = TransformerEncoder(max_len=max_len,dim_model=dim_model,num_heads=num_heads,\n",
    "                       num_layers=num_layers,ffn_hidden=ffn_hidden,vocab_size=src_vocab_size,\n",
    "                       padding_idx=src_padding_idx,drop_prob=drop_prob,device=device)\n",
    "        self.decoder = TransformerDecoder(max_len=max_len,dim_model=dim_model,num_heads=num_heads,\n",
    "                       num_layers=num_layers,ffn_hidden=ffn_hidden,vocab_size=tgt_vocab_size,\n",
    "                       padding_idx=tgt_padding_idx,drop_prob=drop_prob,device=device)\n",
    "        \n",
    "    def forward(self,src,tgt):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        enc_src = self.encoder(src,src_mask)\n",
    "        out = self.decoder(enc_src,tgt,src_mask,tgt_mask)\n",
    "        \n",
    "        return out\n",
    "    # make mask\n",
    "    def make_src_mask(self,src):\n",
    "        src_mask = (src!=self.src_padding_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        return src_mask\n",
    "    \n",
    "    def make_tgt_mask(self,tgt):\n",
    "        tgt_pad_mask = (tgt!=self.tgt_padding_idx).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        tgt_seq_mask = torch.tril(torch.ones(tgt_seq_len,tgt_seq_len)).type(torch.ByteTensor).to(device)\n",
    "        tgt_mask = tgt_pad_mask & tgt_seq_mask\n",
    "        \n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4935c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb3aa5",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c888d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make mask\n",
    "def make_src_mask(src,src_pad_idx):\n",
    "    src_mask = (src!=src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    return src_mask\n",
    "def make_tgt_mask(tgt,tgt_pad_idx):\n",
    "    tgt_pad_mask = (tgt!=tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    tgt_seq_len = tgt.size(1)\n",
    "    tgt_seq_mask = torch.tril(torch.ones(tgt_seq_len,tgt_seq_len)).type(torch.ByteTensor).to(device)\n",
    "    tgt_mask = tgt_pad_mask & tgt_seq_mask\n",
    "    return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a514f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_src = torch.Tensor([\n",
    "    [9,4,3,2,5,6,1,1],\n",
    "    [5,3,2,6,8,4,1,1],\n",
    "    [2,3,4,5,6,7,8,9],\n",
    "    [5,6,7,4,8,6,2,1]\n",
    "])\n",
    "virtual_tgt = torch.Tensor([\n",
    "    [10,6,4,3,9,7,8,5,1,1],\n",
    "    [5,7,4,2,8,6,9,3,1,1],\n",
    "    [3,5,2,6,7,4,11,9,8,10],\n",
    "    [5,7,2,6,4,9,3,5,6,1]\n",
    "])\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "virtual_src,virtual_tgt = virtual_src.long().to(device),virtual_tgt.long().to(device)\n",
    "virtual_src_voc_size = 10\n",
    "virtual_tgt_voc_size = 11\n",
    "virtual_batch_size = 4\n",
    "virtual_dmodel = 6\n",
    "virtual_num_layers = 3\n",
    "virtual_nheads = 2\n",
    "virtual_ffn_hidden = 24\n",
    "virtual_max_len = 100\n",
    "virtual_drop_prob=0.1\n",
    "virtual_src_pad_idx=1\n",
    "virtual_tgt_pad_idx=1\n",
    "virtual_tgt_sos_idx=2\n",
    "#virtual_src_mask = make_src_mask(virtual_src,virtual_src_pad_idx)\n",
    "#virtual_tgt_mask = make_tgt_mask(virtual_tgt,virtual_tgt_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8b04b27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = Transformer(virtual_max_len,virtual_dmodel,virtual_nheads,virtual_num_layers,virtual_ffn_hidden,\n",
    "                  virtual_src_voc_size,virtual_src_pad_idx,virtual_tgt_voc_size,virtual_tgt_pad_idx,\n",
    "                  virtual_drop_prob,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25a457df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torchlearn/lib/python3.9/site-packages/torch/_tensor_str.py:137: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1712608633180/work/aten/src/ATen/native/mps/operations/Indexing.mm:244.)\n",
      "  nonzero_finite_vals = torch.masked_select(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.2804e-01, -6.5257e-01,  5.7021e-01,  2.4681e-01,  8.0556e-02,\n",
       "          -5.4721e-01,  3.5284e-01, -1.0073e+00, -2.7481e-01, -9.5489e-03,\n",
       "          -4.8646e-01],\n",
       "         [-5.9465e-02, -7.1724e-01,  1.0543e+00, -7.3162e-01,  1.8026e-01,\n",
       "           5.6468e-01, -1.0772e-01, -1.0095e+00, -5.5105e-01,  4.2806e-01,\n",
       "           3.4945e-01],\n",
       "         [-2.2491e-01, -8.6839e-01,  7.6564e-01, -2.6864e-01,  2.8544e-02,\n",
       "          -3.9882e-01, -2.5482e-01, -9.5177e-01, -8.8793e-01, -2.0861e-01,\n",
       "          -1.5761e-01],\n",
       "         [-2.0514e-01, -8.9940e-01,  8.0265e-01, -2.7720e-01, -1.0644e-01,\n",
       "           1.9816e-01,  9.3696e-01, -7.9800e-01,  3.6999e-01, -1.4054e-01,\n",
       "           1.8910e-01],\n",
       "         [-2.2643e-02, -2.7954e-01,  3.9777e-01,  1.4855e-02,  7.3926e-02,\n",
       "           1.9396e-01,  4.2662e-01, -7.1609e-01,  7.8967e-02,  4.3733e-01,\n",
       "          -9.7409e-02],\n",
       "         [ 8.1475e-01,  5.6603e-01, -1.2859e+00,  2.2307e-01, -1.1293e+00,\n",
       "          -9.5517e-02,  1.2320e-01,  1.2554e+00,  4.7999e-01, -4.8995e-02,\n",
       "           7.1559e-01],\n",
       "         [ 1.7273e-01, -7.6204e-02,  9.0438e-02, -4.4045e-02, -2.2579e-01,\n",
       "          -3.7657e-02, -1.4753e-01, -2.9418e-01, -3.0029e-01,  4.5285e-01,\n",
       "           1.7527e-01],\n",
       "         [-2.6636e-01, -8.9151e-01,  1.2579e+00, -5.7049e-01,  4.2290e-01,\n",
       "           4.8198e-01,  1.8880e-01, -1.3858e+00, -4.5391e-01,  3.3891e-01,\n",
       "          -2.3089e-02],\n",
       "         [-3.4637e-01, -7.7965e-01,  6.8327e-01,  1.0322e-01,  1.6583e-01,\n",
       "          -5.9308e-01,  3.6050e-02, -1.1184e+00, -6.6844e-01, -1.4762e-01,\n",
       "          -5.4819e-01],\n",
       "         [-1.7625e-01, -6.1436e-01,  4.2327e-01,  8.0013e-02, -3.2470e-01,\n",
       "          -5.4427e-01,  3.1355e-01, -5.6254e-01, -1.3333e-01, -4.0025e-02,\n",
       "           4.5760e-02]],\n",
       "\n",
       "        [[ 1.3157e-03, -2.1107e-02,  2.0025e-02,  4.9202e-01,  4.1477e-01,\n",
       "           4.0017e-01,  1.0751e+00, -7.9965e-01,  6.4823e-01,  3.1788e-01,\n",
       "          -7.9065e-01],\n",
       "         [ 3.4383e-01, -1.7747e-01, -1.2316e-01, -1.8936e-01,  3.9209e-02,\n",
       "           5.2490e-01,  1.9969e-01, -2.3007e-01, -9.7046e-02, -2.0869e-01,\n",
       "          -1.5458e-01],\n",
       "         [-3.4540e-01, -1.2211e+00,  1.5349e+00, -8.3149e-01,  5.3443e-01,\n",
       "           5.0583e-01,  8.1965e-03, -1.5933e+00, -8.0429e-01,  2.6081e-02,\n",
       "          -5.6672e-02],\n",
       "         [-4.4982e-01, -1.0818e+00,  1.3734e+00, -4.1493e-01,  5.7250e-01,\n",
       "           1.4432e-01,  1.4935e-01, -1.6653e+00, -6.9098e-01,  1.0536e-01,\n",
       "          -4.0194e-01],\n",
       "         [ 2.7505e-01,  1.3769e-01, -2.3029e-01,  1.5130e-01, -3.6154e-01,\n",
       "          -7.6548e-02,  2.5682e-02, -2.5477e-02, -1.7224e-02,  4.6069e-01,\n",
       "           1.7496e-01],\n",
       "         [ 3.5828e-01,  7.3546e-02, -1.3541e-01, -9.6534e-02, -4.2699e-01,\n",
       "           3.9348e-01,  3.9082e-01,  8.3581e-02,  4.0418e-01,  4.7410e-01,\n",
       "           5.1356e-01],\n",
       "         [ 2.1897e-02, -2.9971e-01,  6.2349e-01, -2.0320e-01,  8.7007e-01,\n",
       "           1.1823e+00,  7.0217e-01, -1.2687e+00,  1.5192e-01,  4.2492e-01,\n",
       "          -6.6619e-01],\n",
       "         [-4.4694e-01, -1.1748e+00,  1.5265e+00, -5.8733e-01,  5.9943e-01,\n",
       "           3.9436e-01,  2.8712e-01, -1.7140e+00, -5.6320e-01,  1.2588e-01,\n",
       "          -2.5314e-01],\n",
       "         [-1.5415e-01, -8.1543e-01,  1.0069e+00, -5.7951e-01,  8.2009e-02,\n",
       "           1.0133e-01, -2.6000e-01, -1.0081e+00, -7.7813e-01,  2.1250e-01,\n",
       "           2.1213e-01],\n",
       "         [-1.4396e-01, -6.8953e-01,  5.6972e-01, -1.5509e-01, -3.3271e-01,\n",
       "          -3.7855e-01,  1.6623e-01, -5.7504e-01, -2.6210e-01,  3.9147e-03,\n",
       "           2.4942e-01]],\n",
       "\n",
       "        [[-3.3476e-01, -8.5202e-01,  8.1020e-01, -4.9516e-03,  5.9140e-02,\n",
       "          -3.6516e-02,  9.6567e-01, -1.0273e+00,  3.1591e-01, -5.1525e-02,\n",
       "          -1.7379e-01],\n",
       "         [-2.0344e-01, -7.8206e-01,  1.0864e+00, -4.9052e-01,  2.7559e-01,\n",
       "           4.2500e-01,  2.5418e-01, -1.1856e+00, -3.0286e-01,  3.5703e-01,\n",
       "           7.3332e-02],\n",
       "         [-3.4390e-01, -6.5236e-01,  7.4851e-01,  1.4779e-01,  3.2997e-01,\n",
       "          -1.8100e-01,  4.8110e-01, -1.2391e+00, -1.8408e-01,  1.6683e-01,\n",
       "          -5.6572e-01],\n",
       "         [ 1.2103e-01, -4.3241e-01,  4.6495e-01, -4.6940e-01, -3.3595e-01,\n",
       "           2.0373e-02, -3.5384e-01, -3.5866e-01, -5.7047e-01,  2.9866e-01,\n",
       "           5.5859e-01],\n",
       "         [ 5.7286e-01, -1.0858e-01,  1.1154e-01, -8.5927e-01, -5.0896e-01,\n",
       "           8.3250e-01, -3.4709e-01,  2.3028e-01, -2.4996e-01,  3.9472e-01,\n",
       "           1.1109e+00],\n",
       "         [-3.5742e-01, -9.9025e-01,  1.0387e+00, -2.5927e-01,  5.4037e-02,\n",
       "          -7.8728e-02,  5.3584e-01, -1.1222e+00, -1.3152e-01, -1.3909e-02,\n",
       "          -2.7046e-03],\n",
       "         [-1.2922e-01, -3.7171e-01,  4.0416e-01,  1.9155e-01,  1.0998e-01,\n",
       "           9.7371e-02,  8.0037e-01, -8.0865e-01,  3.5227e-01,  2.8256e-01,\n",
       "          -2.8802e-01],\n",
       "         [-1.8082e-01, -6.5664e-01,  1.0637e+00, -3.9748e-01,  1.0764e+00,\n",
       "           9.8030e-01,  2.9017e-01, -1.7068e+00, -4.8072e-01,  2.8702e-01,\n",
       "          -8.2373e-01],\n",
       "         [ 3.5723e-01,  1.7210e-01, -2.2241e-01,  4.1183e-04, -3.9595e-01,\n",
       "           1.0865e-01, -4.9148e-02,  5.9631e-02, -2.5764e-02,  5.3043e-01,\n",
       "           3.5465e-01],\n",
       "         [-1.3023e-01, -3.8042e-01,  3.9962e-01,  1.3171e-01,  1.2358e-01,\n",
       "          -3.3500e-01, -7.7494e-02, -8.5668e-01, -5.4850e-01,  2.2673e-01,\n",
       "          -4.0327e-01]],\n",
       "\n",
       "        [[-2.3405e-01, -5.4573e-01,  6.0197e-01,  1.2506e-01,  1.4863e-01,\n",
       "          -2.9258e-02,  6.9036e-01, -9.7637e-01,  1.5025e-01,  2.1817e-01,\n",
       "          -3.1265e-01],\n",
       "         [ 4.1220e-01, -2.4693e-01,  1.1840e-01, -5.7922e-01,  1.9914e-02,\n",
       "           8.3880e-01, -5.2408e-02, -2.3922e-01, -2.8013e-01, -2.4191e-02,\n",
       "           2.2984e-01],\n",
       "         [-4.3763e-01, -9.3968e-01,  1.3170e+00, -2.8563e-01,  9.6479e-01,\n",
       "           4.2442e-01,  2.5925e-01, -1.9089e+00, -6.5374e-01,  1.7708e-01,\n",
       "          -8.5950e-01],\n",
       "         [-1.1467e-02, -7.0864e-01,  9.2092e-01, -7.3516e-01, -6.9618e-03,\n",
       "           2.9287e-01, -4.0625e-01, -8.1449e-01, -8.0437e-01,  2.9789e-01,\n",
       "           4.4669e-01],\n",
       "         [ 3.2729e-01, -4.8361e-01,  6.7185e-01, -1.0514e+00, -1.7347e-01,\n",
       "           9.0168e-01, -3.8742e-01, -3.5092e-01, -5.4182e-01,  3.6967e-01,\n",
       "           9.1981e-01],\n",
       "         [ 6.7918e-02, -5.5764e-01,  9.5765e-01, -7.4508e-01,  5.1666e-01,\n",
       "           1.3196e+00,  4.6674e-01, -1.0833e+00,  2.0584e-02,  4.7821e-01,\n",
       "           1.3390e-01],\n",
       "         [-4.0574e-01, -9.4932e-01,  7.4504e-01,  1.6380e-01,  5.0781e-01,\n",
       "          -4.7597e-02,  9.6474e-01, -1.3599e+00,  5.1461e-02, -4.9488e-01,\n",
       "          -9.3141e-01],\n",
       "         [ 4.2071e-02, -8.5323e-01,  1.1961e+00, -1.0944e+00,  3.6635e-01,\n",
       "           1.1400e+00, -3.0084e-02, -1.0745e+00, -5.2828e-01,  2.5202e-01,\n",
       "           4.2244e-01],\n",
       "         [ 5.0613e-01, -4.3910e-03,  2.7642e-01, -8.0621e-01, -3.5774e-02,\n",
       "           1.0449e+00, -5.1209e-01, -2.1516e-01, -5.0344e-01,  6.9407e-01,\n",
       "           6.7942e-01],\n",
       "         [-6.3753e-02, -7.6884e-01,  1.0332e+00, -7.4918e-01,  7.5023e-02,\n",
       "           3.8088e-01, -2.6791e-01, -9.3827e-01, -7.1274e-01,  3.2422e-01,\n",
       "           4.0641e-01]]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(virtual_src,virtual_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446684a",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657bd8a",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81265364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "# model parameter setting\n",
    "batch_size = 128\n",
    "max_len = 256\n",
    "dim_model = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "ffn_hidden = 2048\n",
    "drop_prob = 0.1\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# optimizer parameter setting\n",
    "init_lr = 1e-5\n",
    "factor = 0.9\n",
    "adam_eps = 5e-9\n",
    "patience = 10\n",
    "warmup = 100\n",
    "epoch = 1000\n",
    "clip = 1.0\n",
    "weight_decay = 5e-4\n",
    "inf = float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bde8e5e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torchlearn/lib/python3.9/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/torchlearn/lib/python3.9/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/torchlearn/lib/python3.9/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/torchlearn/lib/python3.9/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initializing start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torchlearn/lib/python3.9/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n",
      "/opt/anaconda3/envs/torchlearn/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initializing done\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class DataLoaderWrapper:\n",
    "    def __init__(self, tokenize_en, tokenize_de, init_token, eos_token):\n",
    "        self.tokenize_en = tokenize_en\n",
    "        self.tokenize_de = tokenize_de\n",
    "        self.init_token = init_token\n",
    "        self.eos_token = eos_token\n",
    "        print('Dataset initializing start')\n",
    "\n",
    "    def make_dataset(self):\n",
    "        # Load the dataset\n",
    "        train_data, valid_data, test_data = Multi30k(split=('train', 'valid', 'test'))\n",
    "        return train_data, valid_data, test_data\n",
    "\n",
    "    def build_vocab(self, train_data, min_freq):\n",
    "        def yield_tokens(data_iter, tokenizer):\n",
    "            for src, trg in data_iter:\n",
    "                yield tokenizer(src)\n",
    "                yield tokenizer(trg)\n",
    "\n",
    "        self.src_vocab = build_vocab_from_iterator(yield_tokens(train_data, self.tokenize_de),\n",
    "                                              min_freq=min_freq, specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
    "        self.src_vocab.set_default_index(self.src_vocab['<unk>'])\n",
    "\n",
    "        self.tgt_vocab = build_vocab_from_iterator(yield_tokens(train_data, self.tokenize_en),\n",
    "                                              min_freq=min_freq, specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
    "        self.tgt_vocab.set_default_index(self.tgt_vocab['<unk>'])\n",
    "\n",
    "        return self.src_vocab, self.tgt_vocab\n",
    "\n",
    "    def collate_batch(self, batch):\n",
    "        src_batch, tgt_batch = zip(*batch)\n",
    "        src_batch = [torch.tensor(self.src_vocab(self.tokenize_de(x))) for x in src_batch]\n",
    "        tgt_batch = [torch.tensor(self.tgt_vocab(self.tokenize_en(x))) for x in tgt_batch]\n",
    "\n",
    "        src_batch = pad_sequence(src_batch, padding_value=self.src_vocab['<pad>'],batch_first=True)\n",
    "        tgt_batch = pad_sequence(tgt_batch, padding_value=self.tgt_vocab['<pad>'],batch_first=True)\n",
    "\n",
    "        return src_batch, tgt_batch\n",
    "\n",
    "    def make_iter(self, train_data, valid_data, test_data, batch_size, device):\n",
    "        train_iter = DataLoader(train_data, batch_size=batch_size, collate_fn=self.collate_batch, shuffle=True)\n",
    "        valid_iter = DataLoader(valid_data, batch_size=batch_size, collate_fn=self.collate_batch)\n",
    "        test_iter = DataLoader(test_data, batch_size=batch_size, collate_fn=self.collate_batch)\n",
    "        print('Dataset initializing done')\n",
    "        return train_iter, valid_iter, test_iter\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.spacy_de = spacy.load('de_core_news_sm')\n",
    "        self.spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def tokenize_de(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes German text from a string into a list of strings\n",
    "        \"\"\"\n",
    "        return [tok.text for tok in self.spacy_de.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes English text from a string into a list of strings\n",
    "        \"\"\"\n",
    "        return [tok.text for tok in self.spacy_en.tokenizer(text)]\n",
    "\n",
    "# Example usage\n",
    "tokenizer = Tokenizer()\n",
    "loader = DataLoaderWrapper(\n",
    "    tokenize_en=tokenizer.tokenize_en,\n",
    "    tokenize_de=tokenizer.tokenize_de,\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>'\n",
    ")\n",
    "\n",
    "train, valid, test = loader.make_dataset()\n",
    "src_vocab, tgt_vocab = loader.build_vocab(train_data=train, min_freq=2)\n",
    "\n",
    "\n",
    "train_iter, valid_iter, test_iter = loader.make_iter(train, valid, test,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     device=device)\n",
    "# data parameters\n",
    "src_pad_idx = src_vocab['<pad>']\n",
    "tgt_pad_idx = tgt_vocab['<pad>']\n",
    "tgt_sos_idx = tgt_vocab['<sos>']\n",
    "\n",
    "enc_voc_size = len(src_vocab)\n",
    "dec_voc_size = len(tgt_vocab)\n",
    "\n",
    "len_train_iter = sum(1 for _ in train_iter)\n",
    "#len_test_iter = sum(1 for _ in test_iter)\n",
    "len_val_iter = sum(1 for _ in valid_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5c007b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 66,472,633 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1z/d7jncgd13d95z730clvn910c0000gn/T/ipykernel_23371/403790165.py:7: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n",
      "/opt/anaconda3/envs/torchlearn/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "\n",
    "\n",
    "model = Transformer(max_len,dim_model,num_heads,num_layers,ffn_hidden,enc_voc_size,src_pad_idx,dec_voc_size,\n",
    "                    tgt_pad_idx,drop_prob,device).to(device)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model.apply(initialize_weights)\n",
    "optimizer = optim.Adam(params=model.parameters(),\n",
    "                 lr=init_lr,\n",
    "                 weight_decay=weight_decay,\n",
    "                 eps=adam_eps)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 verbose=True,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, tgt) in enumerate(iterator):\n",
    "        src,tgt = src.to(device),tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output_reshape, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        #len_iterator=227\n",
    "        #break\n",
    "        print('step :', round((i / len_train_iter) * 100, 2), '% , loss :', loss.item())\n",
    "\n",
    "    return epoch_loss / len_train_iter\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    batch_bleu = []\n",
    "    with torch.no_grad():\n",
    "        for i, (src, tgt) in enumerate(iterator):\n",
    "            src,tgt = src.to(device),tgt.to(device)\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            total_bleu = []\n",
    "            for j in range(tgt.shape[0]):\n",
    "                try:\n",
    "                    tgt_words = idx_to_word(tgt[j], tgt_vocab)\n",
    "                    output_words = output[j].max(dim=1)[1]\n",
    "                    output_words = idx_to_word(output_words, tgt_vocab)\n",
    "                    bleu = get_bleu(hypotheses=output_words.split(), reference=tgt_words.split())\n",
    "                    total_bleu.append(bleu)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    pass\n",
    "\n",
    "            total_bleu = sum(total_bleu) / len(total_bleu)\n",
    "            batch_bleu.append(total_bleu)\n",
    "            \n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output_reshape, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    batch_bleu = sum(batch_bleu) / len(batch_bleu)\n",
    "    return epoch_loss / len_val_iter, batch_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36716f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def idx_to_word(x, vocab):\n",
    "    words = []\n",
    "    for i in x:\n",
    "        word = vocab.get_itos()[i]\n",
    "        if '<' not in word:\n",
    "            words.append(word)\n",
    "    words = \" \".join(words)\n",
    "    return words\n",
    "\n",
    "def get_bleu(hypotheses, reference):\n",
    "    \"\"\"Get validation BLEU score for dev set.\"\"\"\n",
    "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    for hyp, ref in zip(hypotheses, reference):\n",
    "        stats += np.array(bleu_stats(hyp, ref))\n",
    "    return 100 * bleu(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3daea43b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torchlearn/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0.0 % , loss : 10.579936027526855\n",
      "step : 0.44 % , loss : 10.550848007202148\n",
      "step : 0.88 % , loss : 10.5911865234375\n",
      "step : 1.32 % , loss : 10.553374290466309\n",
      "step : 1.76 % , loss : 10.47957992553711\n",
      "step : 2.2 % , loss : 10.550291061401367\n",
      "step : 2.64 % , loss : 10.464485168457031\n",
      "step : 3.08 % , loss : 10.503978729248047\n",
      "step : 3.52 % , loss : 10.500747680664062\n",
      "step : 3.96 % , loss : 10.517138481140137\n",
      "step : 4.41 % , loss : 10.474889755249023\n",
      "step : 4.85 % , loss : 10.471578598022461\n",
      "step : 5.29 % , loss : 10.437241554260254\n",
      "step : 5.73 % , loss : 10.37551498413086\n",
      "step : 6.17 % , loss : 10.390289306640625\n",
      "step : 6.61 % , loss : 10.425599098205566\n",
      "step : 7.05 % , loss : 10.3930082321167\n",
      "step : 7.49 % , loss : 10.3561372756958\n",
      "step : 7.93 % , loss : 10.35879898071289\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mBLEU Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(total_epoch, best_loss)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_epoch):\n\u001b[1;32m      4\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     valid_loss, bleu \u001b[38;5;241m=\u001b[39m evaluate(model, valid_iter, criterion)\n\u001b[1;32m      7\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     36\u001b[0m tgt \u001b[38;5;241m=\u001b[39m tgt[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output_reshape, tgt)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torchlearn/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torchlearn/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torchlearn/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run(total_epoch, best_loss):\n",
    "    train_losses, test_losses, bleus = [], [], []\n",
    "    for step in range(total_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss = train(model, train_iter, optimizer, criterion, clip)\n",
    "        valid_loss, bleu = evaluate(model, valid_iter, criterion)\n",
    "        end_time = time.time()\n",
    "\n",
    "        if step > warmup:\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "        bleus.append(bleu)\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f'saved/model_{step+1}-{valid_loss}.pt')\n",
    "\n",
    "        f = open('result/train_loss.txt', 'w')\n",
    "        f.write(str(train_losses))\n",
    "        f.close()\n",
    "\n",
    "        f = open('result/bleu.txt', 'w')\n",
    "        f.write(str(bleus))\n",
    "        f.close()\n",
    "\n",
    "        f = open('result/test_loss.txt', 'w')\n",
    "        f.write(str(test_losses))\n",
    "        f.close()\n",
    "\n",
    "        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {math.exp(valid_loss):7.3f}')\n",
    "        print(f'\\tBLEU Score: {bleu:.3f}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run(total_epoch=epoch, best_loss=inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d59c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchlearn] *",
   "language": "python",
   "name": "conda-env-torchlearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
